name: Example full-stack-fastapi

on:
  workflow_dispatch:
  issue_comment:
    types: [created]

# Configuration is now managed via config file created during workflow

jobs:
  example-full-stack-fastapi:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event_name == 'workflow_dispatch' || (github.event_name == 'issue_comment' && github.event.issue.pull_request && contains(github.event.comment.body, 'run test generation'))

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        # For PR comments, checkout the PR branch
        ref: ${{ github.event_name == 'issue_comment' && format('refs/pull/{0}/head', github.event.issue.number) || github.ref }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.13'

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    - name: Install dependencies
      run: |
        uv sync --group dev
        echo "âœ… Dependencies installed successfully"

    - name: Validate configuration
      run: |
        echo "ðŸ”§ Validating diversifier configuration..."
        echo "Will create config with:"
        echo "LLM Provider: google_genai"
        echo "Model: gemini-2.5-flash"
        echo "Max Tokens: 200000"
        echo "Temperature: 0.2"
        echo "API Key Environment Variable: GOOGLE_API_KEY"
        
        # Check if API key is available (without exposing it)
        if [ -z "$GOOGLE_API_KEY" ]; then
          echo "âŒ GOOGLE_API_KEY not found in environment"
          exit 1
        else
          echo "âœ… Google API key is available"
        fi
      env:
        GOOGLE_API_KEY: ${{ secrets.GEMINI_API_KEY }}

    - name: Clone FastAPI Full-Stack Template pilot project
      run: |
        echo "ðŸ“ Cloning FastAPI Full-Stack Template pilot project..."
        git clone https://github.com/fastapi/full-stack-fastapi-template.git pilot-project
        
        echo "âœ… Pilot project cloned successfully"
        echo "Project structure:"
        ls -la pilot-project/
        
        echo "Checking for emails library dependency..."
        if find pilot-project -name "*.txt" -o -name "*.toml" -o -name "*.lock" | xargs grep -l "emails" 2>/dev/null; then
          echo "âœ… Found emails library in project dependencies"
        else
          echo "âš ï¸  emails library not found in dependencies - this is expected as dependencies may have changed"
        fi

    - name: Create diversifier config
      run: |
        echo "ðŸ”§ Creating diversifier configuration file..."
        
        touch diversifier_config.toml
        echo '[logging]' >> diversifier_config.toml
        echo 'level = "DEBUG"' >> diversifier_config.toml
        echo 'format_string = "%(asctime)s | %(levelname)-8s | %(name)-25s | %(message)s"' >> diversifier_config.toml
        echo '' >> diversifier_config.toml
        echo '[migration]' >> diversifier_config.toml
        echo 'test_paths = ["app/tests"]' >> diversifier_config.toml
        echo '' >> diversifier_config.toml
        echo '[llm]' >> diversifier_config.toml
        echo 'provider = "google_genai"' >> diversifier_config.toml
        echo 'model_name = "gemini-2.5-flash"' >> diversifier_config.toml
        echo 'api_key_env_var = "GOOGLE_API_KEY"' >> diversifier_config.toml
        echo 'temperature = 0.1' >> diversifier_config.toml
        echo 'max_tokens = 200000' >> diversifier_config.toml
        echo 'retryable_error_codes = [429, 503]' >> diversifier_config.toml
      
        if [ -f "diversifier_config.toml" ]; then
          echo "Config file location: diversifier_config.toml"
          echo "Initial config contents:"
          cat diversifier_config.toml
        else
          echo "âŒ Configuration file not found"
          exit 1
        fi
        
    - name: Run the tool
      run: |
        echo "ðŸ§ª Running diversifier for acceptance test generation using config file..."
        
        # Set up environment for test generation
        export GOOGLE_API_KEY="${{ secrets.GEMINI_API_KEY }}"
        
        # Set up LangSmith tracing if API key is available
        if [ -n "$LANGSMITH_API_KEY" ]; then
          echo "âœ… Enabling LangSmith tracing"
          export LANGSMITH_TRACING="true"
          export LANGSMITH_ENDPOINT="https://api.smith.langchain.com"
          export LANGSMITH_PROJECT="pr-pertinent-youth-30"
        else
          echo "âš ï¸  LangSmith API key not found, tracing disabled"
        fi
        
        # Use config file approach
        echo "Starting test generation workflow with config file..."
        
        set +e  # Don't fail immediately on error so we can capture partial results
        
        # Run diversifier using config file
        uv run diversifier pilot-project/backend emails redmail --config diversifier_config.toml --verbose 2>&1 | tee diversifier_output.log
        
        DIVERSIFIER_EXIT_CODE=$?
        
        echo "Diversifier exit code: $DIVERSIFIER_EXIT_CODE"
        
        # Check if acceptance tests were generated regardless of exit code
        if [ -d "pilot-project/acceptance_tests" ] || [ -d "acceptance_tests" ]; then
          echo "âœ… Test generation directory found"
          TESTS_GENERATED=true
        else
          echo "âš ï¸  Test generation directory not found, checking for other test outputs..."
          find . -name "*test*" -type f -newer pilot-project 2>/dev/null | head -10
          TESTS_GENERATED=false
        fi
        
        # Log key information for debugging
        echo "=== Diversifier Output Analysis ==="
        if grep -i "test.*generat" diversifier_output.log; then
          echo "âœ… Found test generation activity in logs"
        fi
        
        if grep -i "error\|fail" diversifier_output.log; then
          echo "âš ï¸  Found errors in diversifier output:"
          grep -i "error\|fail" diversifier_output.log | head -5
        fi
        
        # Continue even if diversifier had issues - we want to capture artifacts
        echo "Continuing with artifact collection..."
      env:
        GOOGLE_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}

    - name: Analyze generated tests
      run: |
        echo "ðŸ“Š Analyzing test generation results..."
        
        # Look for generated tests in various possible locations
        TEST_LOCATIONS=(
          "pilot-project/acceptance_tests"
          "acceptance_tests" 
          "pilot-project/tests"
          "tests"
          "pilot-project/test_*"
          "test_*"
        )
        
        TESTS_FOUND=false
        for location in "${TEST_LOCATIONS[@]}"; do
          if ls $location 2>/dev/null | head -1 > /dev/null; then
            echo "âœ… Found tests in: $location"
            echo "Contents:"
            ls -la "$location" 2>/dev/null | head -10
            TESTS_FOUND=true
          fi
        done
        
        if [ "$TESTS_FOUND" = true ]; then
          echo "âœ… Acceptance test generation validation: SUCCESSFUL"
          echo "Generated tests are available for analysis"
        else
          echo "âš ï¸  No generated tests found in expected locations"
          echo "This may indicate an issue with test generation or output location"
        fi
        
        # Create a summary report
        echo "=== Test Generation Summary ===" > test_generation_summary.txt
        echo "Timestamp: $(date)" >> test_generation_summary.txt
        echo "Project: FastAPI Full-Stack Template (emails -> redmail)" >> test_generation_summary.txt
        echo "Tests found: $TESTS_FOUND" >> test_generation_summary.txt
        echo "Locations checked:" >> test_generation_summary.txt
        for location in "${TEST_LOCATIONS[@]}"; do
          echo "  - $location" >> test_generation_summary.txt
        done

    - name: Store generated tests and logs
      uses: actions/upload-artifact@v4
      if: always()  # Run even if previous steps failed
      with:
        name: test-generation-results
        path: |
          pilot-project/acceptance_tests/
          acceptance_tests/
          pilot-project/tests/
          tests/
          pilot-project/test_*.py
          test_*.py
          diversifier_output.log
          test_generation_summary.txt
          diversifier_config.toml
          pilot-project/
        retention-days: 30

    - name: Upload diversifier logs
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: diversifier-logs
        path: |
          diversifier_output.log
          *.log
          **/*.log
        retention-days: 7

    - name: Report results
      run: |
        echo "ðŸ“‹ Test Generation Validation Report"
        echo "=================================="
        
        if [ -f "test_generation_summary.txt" ]; then
          cat test_generation_summary.txt
        fi
        
        echo ""
        echo "ðŸŽ¯ Validation Objectives:"
        echo "âœ… GitHub Actions workflow executed successfully"
        echo "âœ… Secure API key management verified"
        echo "âœ… Python environment with uv configured"
        echo "âœ… Diversifier tool executed for test generation"
        echo "âœ… Results captured as artifacts"
        
        # The workflow considers itself successful if it ran without critical errors
        # Even if test generation had issues, we want to analyze the artifacts
        echo ""
        echo "ðŸŽ‰ Test generation validation workflow completed!"
        echo "Check the artifacts for detailed analysis of generated tests."
