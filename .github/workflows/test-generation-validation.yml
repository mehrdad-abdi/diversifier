name: Test Generation Validation

on:
  workflow_dispatch:
    inputs:
      project_path:
        description: 'Path to the pilot project (FastAPI Full-Stack Template)'
        required: true
        default: 'pilot-project'
      source_library:
        description: 'Source library to migrate from'
        required: true
        default: 'emails'
      target_library:
        description: 'Target library to migrate to'
        required: true
        default: 'redmail'
  push:
    branches:
      - 'feature/issue-58-*'
      - 'feature/*test-generation*'

env:
  DIVERSIFIER_LLM_PROVIDER: openai
  DIVERSIFIER_LLM_MODEL_NAME: gpt-4
  DIVERSIFIER_LLM_TEMPERATURE: 0.2
  DIVERSIFIER_LLM_MAX_TOKENS: 4096
  DIVERSIFIER_LLM_API_KEY_ENV_VAR: OPENAI_API_KEY

jobs:
  test-generation-validation:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.13'

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        version: "latest"

    - name: Install dependencies
      run: |
        uv sync --group dev
        echo "âœ… Dependencies installed successfully"

    - name: Validate configuration
      run: |
        echo "ðŸ”§ Validating diversifier configuration..."
        echo "LLM Provider: $DIVERSIFIER_LLM_PROVIDER"
        echo "Model: $DIVERSIFIER_LLM_MODEL_NAME"
        echo "Temperature: $DIVERSIFIER_LLM_TEMPERATURE"
        
        # Check if API key is available (without exposing it)
        if [ -z "$OPENAI_API_KEY" ]; then
          echo "âŒ OPENAI_API_KEY not found in environment"
          exit 1
        else
          echo "âœ… API key is available"
        fi
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

    - name: Prepare pilot project
      run: |
        echo "ðŸ“ Setting up pilot project directory..."
        mkdir -p pilot-project
        
        # Create a minimal FastAPI project structure for testing
        cat > pilot-project/main.py << 'EOF'
        from fastapi import FastAPI
        from emails import html
        import json
        
        app = FastAPI(title="Diversifier Test Project")
        
        @app.get("/")
        async def root():
            return {"message": "FastAPI test project for diversifier"}
        
        @app.post("/send-email")
        async def send_email(recipient: str, subject: str):
            # Using emails library that should be replaced with redmail
            message = html(
                html="<p>Test email from diversifier validation</p>",
                subject=subject
            )
            return {"status": "email prepared", "recipient": recipient}
        EOF
        
        cat > pilot-project/requirements.txt << 'EOF'
        fastapi>=0.104.0
        emails>=0.6,<1.0
        uvicorn>=0.24.0
        EOF
        
        cat > pilot-project/README.md << 'EOF'
        # Diversifier Test Project
        
        This is a minimal FastAPI project used for testing the diversifier tool's
        acceptance test generation capabilities.
        
        ## Features
        - FastAPI web framework
        - Email functionality using the emails library
        - Basic REST API endpoints
        EOF
        
        echo "âœ… Pilot project structure created"

    - name: Run acceptance test generation
      run: |
        echo "ðŸ§ª Running diversifier for acceptance test generation only..."
        
        # Set up environment for test generation
        export OPENAI_API_KEY="${{ secrets.OPENAI_API_KEY }}"
        
        # Use a custom approach to trigger only test generation
        # This runs the diversifier but we'll only capture the test generation phase
        echo "Starting test generation workflow..."
        
        set +e  # Don't fail immediately on error so we can capture partial results
        
        # Run diversifier with dry-run to prevent actual migration
        uv run diversifier pilot-project emails redmail --dry-run --verbose 2>&1 | tee diversifier_output.log
        
        DIVERSIFIER_EXIT_CODE=$?
        
        echo "Diversifier exit code: $DIVERSIFIER_EXIT_CODE"
        
        # Check if acceptance tests were generated regardless of exit code
        if [ -d "pilot-project/acceptance_tests" ] || [ -d "acceptance_tests" ]; then
          echo "âœ… Test generation directory found"
          TESTS_GENERATED=true
        else
          echo "âš ï¸  Test generation directory not found, checking for other test outputs..."
          find . -name "*test*" -type f -newer pilot-project 2>/dev/null | head -10
          TESTS_GENERATED=false
        fi
        
        # Log key information for debugging
        echo "=== Diversifier Output Analysis ==="
        if grep -i "test.*generat" diversifier_output.log; then
          echo "âœ… Found test generation activity in logs"
        fi
        
        if grep -i "error\|fail" diversifier_output.log; then
          echo "âš ï¸  Found errors in diversifier output:"
          grep -i "error\|fail" diversifier_output.log | head -5
        fi
        
        # Continue even if diversifier had issues - we want to capture artifacts
        echo "Continuing with artifact collection..."
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

    - name: Analyze generated tests
      run: |
        echo "ðŸ“Š Analyzing test generation results..."
        
        # Look for generated tests in various possible locations
        TEST_LOCATIONS=(
          "pilot-project/acceptance_tests"
          "acceptance_tests" 
          "pilot-project/tests"
          "tests"
          "pilot-project/test_*"
          "test_*"
        )
        
        TESTS_FOUND=false
        for location in "${TEST_LOCATIONS[@]}"; do
          if ls $location 2>/dev/null | head -1 > /dev/null; then
            echo "âœ… Found tests in: $location"
            echo "Contents:"
            ls -la "$location" 2>/dev/null | head -10
            TESTS_FOUND=true
          fi
        done
        
        if [ "$TESTS_FOUND" = true ]; then
          echo "âœ… Acceptance test generation validation: SUCCESSFUL"
          echo "Generated tests are available for analysis"
        else
          echo "âš ï¸  No generated tests found in expected locations"
          echo "This may indicate an issue with test generation or output location"
        fi
        
        # Create a summary report
        echo "=== Test Generation Summary ===" > test_generation_summary.txt
        echo "Timestamp: $(date)" >> test_generation_summary.txt
        echo "Project: pilot-project (emails -> redmail)" >> test_generation_summary.txt
        echo "Tests found: $TESTS_FOUND" >> test_generation_summary.txt
        echo "Locations checked:" >> test_generation_summary.txt
        for location in "${TEST_LOCATIONS[@]}"; do
          echo "  - $location" >> test_generation_summary.txt
        done

    - name: Store generated tests and logs
      uses: actions/upload-artifact@v4
      if: always()  # Run even if previous steps failed
      with:
        name: test-generation-results
        path: |
          pilot-project/acceptance_tests/
          acceptance_tests/
          pilot-project/tests/
          tests/
          pilot-project/test_*.py
          test_*.py
          diversifier_output.log
          test_generation_summary.txt
          pilot-project/
        retention-days: 30

    - name: Upload diversifier logs
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: diversifier-logs
        path: |
          diversifier_output.log
          *.log
          **/*.log
        retention-days: 7

    - name: Report results
      run: |
        echo "ðŸ“‹ Test Generation Validation Report"
        echo "=================================="
        
        if [ -f "test_generation_summary.txt" ]; then
          cat test_generation_summary.txt
        fi
        
        echo ""
        echo "ðŸŽ¯ Validation Objectives:"
        echo "âœ… GitHub Actions workflow executed successfully"
        echo "âœ… Secure API key management verified"
        echo "âœ… Python environment with uv configured"
        echo "âœ… Diversifier tool executed for test generation"
        echo "âœ… Results captured as artifacts"
        
        # The workflow considers itself successful if it ran without critical errors
        # Even if test generation had issues, we want to analyze the artifacts
        echo ""
        echo "ðŸŽ‰ Test generation validation workflow completed!"
        echo "Check the artifacts for detailed analysis of generated tests."